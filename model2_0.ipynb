{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laber_name = os.listdir('data/train/')\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit_transform(laber_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(44)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emotion(Dataset):\n",
    "    def __init__(self, root, transform=None, train=True):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        if train == True:\n",
    "            self.images = self.getImage()\n",
    "        else:\n",
    "            self.images = self.getImageTest()\n",
    "\n",
    "    def getImage(self):\n",
    "        images = []\n",
    "        emotions = os.listdir(self.root)\n",
    "        for emotion in emotions:\n",
    "            root = os.path.join(self.root, emotion)\n",
    "            imageNames = os.listdir(root)\n",
    "            imagesList = [root +'/' + image for image in imageNames]\n",
    "            images = images + imagesList\n",
    "        return images\n",
    "    \n",
    "    def getImageTest(self):\n",
    "        images = [self.root + '/' + image for image in os.listdir(self.root)]\n",
    "        return images\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image_path = self.images[index]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        image = np.array(image)[:, :, np.newaxis]\n",
    "        image = np.concatenate((image, image, image), axis=2)\n",
    "\n",
    "        data = self.transform(image)\n",
    "        \n",
    "        if self.train == True:\n",
    "            label = encoder.transform([image_path.split('/')[2]])\n",
    "            return data, label\n",
    "        \n",
    "        name = image_path.split('/')[-1]\n",
    "        return data, name\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = Emotion(root='data/train/', transform=trans)\n",
    "trainLoader = DataLoader(trainSet, batch_size=128, shuffle=True)\n",
    "\n",
    "valSet = Emotion(root='data/val/', transform=trans)\n",
    "valLoader = DataLoader(valSet, batch_size=32, shuffle=True)\n",
    "\n",
    "testSet = Emotion(root='data/test/', transform=trans, train=False)\n",
    "testLoader = DataLoader(testSet, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16 = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg)\n",
    "        self.fc = nn.Linear(512, 7)\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            \n",
    "            else:\n",
    "                layers += [\n",
    "                    nn.Conv2d(in_channels, x, kernel_size=3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(x),\n",
    "                    nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        \n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = nn.functional.dropout(out, p=0.5, training=self.training)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionRecognition:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "        self.model = VGG(VGG16).to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = 0.001)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    def train(self, trainLoader, num_epochs, load=True):\n",
    "        if load == True:\n",
    "            self.model.load_state_dict(torch.load('emotion.pth'))\n",
    "            print('load finish')\n",
    "\n",
    "        else:\n",
    "            self.model.train()\n",
    "            for epoch in range(num_epochs):\n",
    "                running_loss = 0.0\n",
    "                for i,data in enumerate(trainLoader, 0):\n",
    "                    images, labels = data[0].to(self.device), data[1].to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    outputs = self.model.forward(images)\n",
    "                    loss = self.criterion(outputs, labels.reshape(-1,).long())\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    if i % 100 == 0:\n",
    "                        print(f'epoch: {epoch + 1}/{num_epochs}, step: {i}/{len(trainLoader)}, loss: {loss.item()}')\n",
    "\n",
    "            torch.save(self.model.state_dict(), 'emotion.pth')\n",
    "            print('finish')\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data[0].to(self.device), data[1].to(self.device)\n",
    "                outputs = self.model.forward(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.reshape(-1,).long().size(0)\n",
    "                correct += (predicted == labels.reshape(-1,).long()).sum().item()\n",
    "        accuracy = correct / total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/30, step: 0/154, loss: 2.320784091949463\n",
      "epoch: 1/30, step: 100/154, loss: 1.8492485284805298\n",
      "epoch: 2/30, step: 0/154, loss: 1.911111831665039\n",
      "epoch: 2/30, step: 100/154, loss: 1.969126582145691\n",
      "epoch: 3/30, step: 0/154, loss: 1.9392344951629639\n",
      "epoch: 3/30, step: 100/154, loss: 1.8171815872192383\n",
      "epoch: 4/30, step: 0/154, loss: 1.8924098014831543\n",
      "epoch: 4/30, step: 100/154, loss: 1.801241397857666\n",
      "epoch: 5/30, step: 0/154, loss: 1.8290725946426392\n",
      "epoch: 5/30, step: 100/154, loss: 1.719276785850525\n",
      "epoch: 6/30, step: 0/154, loss: 1.8464679718017578\n",
      "epoch: 6/30, step: 100/154, loss: 1.6889992952346802\n",
      "epoch: 7/30, step: 0/154, loss: 1.820366621017456\n",
      "epoch: 7/30, step: 100/154, loss: 1.6638925075531006\n",
      "epoch: 8/30, step: 0/154, loss: 1.6818726062774658\n",
      "epoch: 8/30, step: 100/154, loss: 1.560455322265625\n",
      "epoch: 9/30, step: 0/154, loss: 1.479022741317749\n",
      "epoch: 9/30, step: 100/154, loss: 1.4217413663864136\n",
      "epoch: 10/30, step: 0/154, loss: 1.345154047012329\n",
      "epoch: 10/30, step: 100/154, loss: 1.4026213884353638\n",
      "epoch: 11/30, step: 0/154, loss: 1.3997787237167358\n",
      "epoch: 11/30, step: 100/154, loss: 1.3607699871063232\n",
      "epoch: 12/30, step: 0/154, loss: 1.2614139318466187\n",
      "epoch: 12/30, step: 100/154, loss: 1.304949164390564\n",
      "epoch: 13/30, step: 0/154, loss: 1.2569060325622559\n",
      "epoch: 13/30, step: 100/154, loss: 1.3171466588974\n",
      "epoch: 14/30, step: 0/154, loss: 1.257124423980713\n",
      "epoch: 14/30, step: 100/154, loss: 1.3029651641845703\n",
      "epoch: 15/30, step: 0/154, loss: 1.175440788269043\n",
      "epoch: 15/30, step: 100/154, loss: 1.312041163444519\n",
      "epoch: 16/30, step: 0/154, loss: 1.2344590425491333\n",
      "epoch: 16/30, step: 100/154, loss: 1.3041762113571167\n",
      "epoch: 17/30, step: 0/154, loss: 1.1471868753433228\n",
      "epoch: 17/30, step: 100/154, loss: 1.0682610273361206\n",
      "epoch: 18/30, step: 0/154, loss: 1.0647425651550293\n",
      "epoch: 18/30, step: 100/154, loss: 0.9728179574012756\n",
      "epoch: 19/30, step: 0/154, loss: 1.0911743640899658\n",
      "epoch: 19/30, step: 100/154, loss: 1.0679820775985718\n",
      "epoch: 20/30, step: 0/154, loss: 1.0312769412994385\n",
      "epoch: 20/30, step: 100/154, loss: 1.0834091901779175\n",
      "epoch: 21/30, step: 0/154, loss: 1.1170539855957031\n",
      "epoch: 21/30, step: 100/154, loss: 0.9308137893676758\n",
      "epoch: 22/30, step: 0/154, loss: 0.9079355001449585\n",
      "epoch: 22/30, step: 100/154, loss: 0.9911659955978394\n",
      "epoch: 23/30, step: 0/154, loss: 1.0707811117172241\n",
      "epoch: 23/30, step: 100/154, loss: 1.0313645601272583\n",
      "epoch: 24/30, step: 0/154, loss: 1.0024564266204834\n",
      "epoch: 24/30, step: 100/154, loss: 0.9145177006721497\n",
      "epoch: 25/30, step: 0/154, loss: 0.9568803906440735\n",
      "epoch: 25/30, step: 100/154, loss: 0.9122157096862793\n",
      "epoch: 26/30, step: 0/154, loss: 0.9031756520271301\n",
      "epoch: 26/30, step: 100/154, loss: 0.8089280128479004\n",
      "epoch: 27/30, step: 0/154, loss: 0.7052502632141113\n",
      "epoch: 27/30, step: 100/154, loss: 1.056627631187439\n",
      "epoch: 28/30, step: 0/154, loss: 0.9204825162887573\n",
      "epoch: 28/30, step: 100/154, loss: 0.8663280010223389\n",
      "epoch: 29/30, step: 0/154, loss: 0.7159020900726318\n",
      "epoch: 29/30, step: 100/154, loss: 0.9152935147285461\n",
      "epoch: 30/30, step: 0/154, loss: 0.7368295788764954\n",
      "epoch: 30/30, step: 100/154, loss: 0.8800975680351257\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "emotion = EmotionRecognition()\n",
    "emotion.train(trainLoader=trainLoader, num_epochs=0, load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6214470284237726"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion.evaluate(test_loader=valLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "file_name = []\n",
    "\n",
    "for images, name in testLoader:\n",
    "    images = images.to(emotion.device)\n",
    "    emotion.model.eval()\n",
    "    pred = pred + torch.max(emotion.model.forward(images), 1)[1].tolist()\n",
    "    file_name.append(name[0])\n",
    "pred = encoder.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'files_name' : file_name, 'class' : pred }, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('epoch30.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
