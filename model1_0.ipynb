{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.数据集准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1处理标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过sklearn的LaberEncoder对图像的标签进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laber_name = os.listdir('data/train/')\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit_transform(laber_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2处理图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据集中图片转化为Dataset类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emotion(Dataset):\n",
    "    def __init__(self, root, transform=None, train=True):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        if train == True:\n",
    "            self.images = self.getImage()\n",
    "        else:\n",
    "            self.images = self.getImageTest()\n",
    "\n",
    "    def getImage(self):\n",
    "        images = []\n",
    "        emotions = os.listdir(self.root)\n",
    "        for emotion in emotions:\n",
    "            root = os.path.join(self.root, emotion)\n",
    "            imageNames = os.listdir(root)\n",
    "            imagesList = [root +'/' + image for image in imageNames]\n",
    "            images = images + imagesList\n",
    "        return images\n",
    "    \n",
    "    def getImageTest(self):\n",
    "        images = [self.root + '/' + image for image in os.listdir(self.root)]\n",
    "        return images\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image_path = self.images[index]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        data = self.transform(image)\n",
    "        \n",
    "        if self.train == True:\n",
    "            label = encoder.transform([image_path.split('/')[2]])\n",
    "            return data, label\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3647, -0.2392, -0.4745,  ..., -0.2000, -0.0510, -0.1765],\n",
       "          [-0.2314, -0.2000, -0.5294,  ..., -0.0353,  0.0118, -0.0039],\n",
       "          [-0.1608, -0.2863, -0.4431,  ..., -0.0353,  0.1294,  0.0275],\n",
       "          ...,\n",
       "          [-0.2706, -0.1529, -0.1059,  ...,  0.0824,  0.0275,  0.0353],\n",
       "          [-0.4431, -0.5294, -0.4510,  ...,  0.0667,  0.0588,  0.0353],\n",
       "          [-0.9373, -1.0000, -1.0000,  ...,  0.0588,  0.0353,  0.0196]]]),\n",
       " array([0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion = Emotion(root='data/train/', transform=trans)\n",
    "emotion[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3数据集准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loader 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Emotion(root='data/train/', transform=trans)\n",
    "trainLoader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = Emotion(root='data/test/', transform=trans, train=False)\n",
    "testLoader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "val_dataset = Emotion(root='data/val/', transform=trans)\n",
    "valLoader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super(EmotionCNN, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(9 * 9 * 16, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.fc4 = nn.Linear(128, 7)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 9 * 9 * 16)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionRecognition:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "        self.model = EmotionCNN().to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = 0.001)\n",
    "\n",
    "    def train(self, trainLoader, num_epochs, load=True):\n",
    "        if load == True:\n",
    "            self.model.load_state_dict(torch.load('emotion.pth'))\n",
    "            print('load finish')\n",
    "\n",
    "        else:\n",
    "            self.model.train()\n",
    "            for epoch in range(num_epochs):\n",
    "                running_loss = 0.0\n",
    "                for i,data in enumerate(trainLoader, 0):\n",
    "                    images, labels = data[0].to(self.device), data[1].to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    outputs = self.model.forward(images)\n",
    "                    loss = self.criterion(outputs, labels.reshape(-1,).long())\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    if i % 100 == 0:\n",
    "                        print(f'epoch: {epoch + 1}/{num_epochs}, step: {i}/{len(trainLoader)}, loss: {loss.item()}')\n",
    "\n",
    "            torch.save(self.model.state_dict(), 'emotion.pth')\n",
    "            print('finish')\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data[0].to(self.device), data[1].to(self.device)\n",
    "                outputs = self.model.forward(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.reshape(-1,).long().size(0)\n",
    "                correct += (predicted == labels.reshape(-1,).long()).sum().item()\n",
    "        accuracy = correct / total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/15, step: 0/308, loss: 1.94259512424469\n",
      "epoch: 1/15, step: 100/308, loss: 1.8010259866714478\n",
      "epoch: 1/15, step: 200/308, loss: 1.6210118532180786\n",
      "epoch: 1/15, step: 300/308, loss: 1.6426857709884644\n",
      "epoch: 2/15, step: 0/308, loss: 1.509759783744812\n",
      "epoch: 2/15, step: 100/308, loss: 1.3593642711639404\n",
      "epoch: 2/15, step: 200/308, loss: 1.357539415359497\n",
      "epoch: 2/15, step: 300/308, loss: 1.5541982650756836\n",
      "epoch: 3/15, step: 0/308, loss: 1.3315808773040771\n",
      "epoch: 3/15, step: 100/308, loss: 1.3869534730911255\n",
      "epoch: 3/15, step: 200/308, loss: 1.342908501625061\n",
      "epoch: 3/15, step: 300/308, loss: 1.442569375038147\n",
      "epoch: 4/15, step: 0/308, loss: 1.3510205745697021\n",
      "epoch: 4/15, step: 100/308, loss: 1.2106720209121704\n",
      "epoch: 4/15, step: 200/308, loss: 1.1395156383514404\n",
      "epoch: 4/15, step: 300/308, loss: 1.2936512231826782\n",
      "epoch: 5/15, step: 0/308, loss: 1.0547114610671997\n",
      "epoch: 5/15, step: 100/308, loss: 1.0763795375823975\n",
      "epoch: 5/15, step: 200/308, loss: 0.9711562395095825\n",
      "epoch: 5/15, step: 300/308, loss: 1.2316019535064697\n",
      "epoch: 6/15, step: 0/308, loss: 1.352757215499878\n",
      "epoch: 6/15, step: 100/308, loss: 1.221356987953186\n",
      "epoch: 6/15, step: 200/308, loss: 0.921062171459198\n",
      "epoch: 6/15, step: 300/308, loss: 1.040004014968872\n",
      "epoch: 7/15, step: 0/308, loss: 1.1671812534332275\n",
      "epoch: 7/15, step: 100/308, loss: 0.8221173286437988\n",
      "epoch: 7/15, step: 200/308, loss: 0.9733617901802063\n",
      "epoch: 7/15, step: 300/308, loss: 1.1097633838653564\n",
      "epoch: 8/15, step: 0/308, loss: 0.9104583859443665\n",
      "epoch: 8/15, step: 100/308, loss: 0.9193877577781677\n",
      "epoch: 8/15, step: 200/308, loss: 0.8290019035339355\n",
      "epoch: 8/15, step: 300/308, loss: 0.9608965516090393\n",
      "epoch: 9/15, step: 0/308, loss: 0.8674268126487732\n",
      "epoch: 9/15, step: 100/308, loss: 0.5905933976173401\n",
      "epoch: 9/15, step: 200/308, loss: 0.5188956260681152\n",
      "epoch: 9/15, step: 300/308, loss: 0.7222510576248169\n",
      "epoch: 10/15, step: 0/308, loss: 0.6247901320457458\n",
      "epoch: 10/15, step: 100/308, loss: 0.6212176084518433\n",
      "epoch: 10/15, step: 200/308, loss: 0.6612764596939087\n",
      "epoch: 10/15, step: 300/308, loss: 0.9720161557197571\n",
      "epoch: 11/15, step: 0/308, loss: 0.624356746673584\n",
      "epoch: 11/15, step: 100/308, loss: 0.6631277799606323\n",
      "epoch: 11/15, step: 200/308, loss: 0.8184789419174194\n",
      "epoch: 11/15, step: 300/308, loss: 0.5900804400444031\n",
      "epoch: 12/15, step: 0/308, loss: 0.4754721522331238\n",
      "epoch: 12/15, step: 100/308, loss: 0.4438250660896301\n",
      "epoch: 12/15, step: 200/308, loss: 0.5360553860664368\n",
      "epoch: 12/15, step: 300/308, loss: 0.46677589416503906\n",
      "epoch: 13/15, step: 0/308, loss: 0.47952795028686523\n",
      "epoch: 13/15, step: 100/308, loss: 0.29513224959373474\n",
      "epoch: 13/15, step: 200/308, loss: 0.6319754123687744\n",
      "epoch: 13/15, step: 300/308, loss: 0.48762786388397217\n",
      "epoch: 14/15, step: 0/308, loss: 0.4473736584186554\n",
      "epoch: 14/15, step: 100/308, loss: 0.4875115752220154\n",
      "epoch: 14/15, step: 200/308, loss: 0.4549983739852905\n",
      "epoch: 14/15, step: 300/308, loss: 0.35444340109825134\n",
      "epoch: 15/15, step: 0/308, loss: 0.5217385292053223\n",
      "epoch: 15/15, step: 100/308, loss: 0.44097936153411865\n",
      "epoch: 15/15, step: 200/308, loss: 0.3018779158592224\n",
      "epoch: 15/15, step: 300/308, loss: 0.37095391750335693\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "emotion = EmotionRecognition()\n",
    "emotion.train(trainLoader=trainLoader, num_epochs=15, load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5116279069767442"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion.evaluate(test_loader=valLoader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
